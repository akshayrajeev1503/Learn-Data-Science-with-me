In a transformer, the Query (Q), Key (K), and Value (V) matrices are generated by multiplying the input sequence's embeddings with three separate, randomly initialized weight matrices (Wq, Wk, Wv). Initialization Process Input Embedding: The input words or tokens are first converted into dense numerical vectors called embeddings. Positional encodings are added to these embeddings to incorporate information about the sequence order.Weight Matrices: Three distinct weight matrices, \(W_{q}\), \(W_{k}\), and \(W_{v}\), are created for each attention head in the transformer architecture.These weight matrices are initialized with small, random values before training begins. Common initialization methods like Xavier/Glorot are often used to ensure stable training.These are learnable parameters, meaning their values are adjusted during the model's training process via backpropagation.Projection: The Q, K, and V matrices for the attention mechanism are then computed by multiplying the input embeddings matrix (\(X\)) by their respective randomly initialized weight matrices:\(Q=X\cdot W_{q}\)\(K=X\cdot W_{k}\)\(V=X\cdot W_{v}\) This process projects the input embeddings into three different representational subspaces, allowing the attention mechanism to learn what information to look for (Query), what information each token offers (Key), and the content to be retrieved (Value). 
